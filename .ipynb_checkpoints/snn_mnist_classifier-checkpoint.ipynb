{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN MNIST Classifier ğŸ“Š\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” snnTorchë¥¼ ì‚¬ìš©í•˜ì—¬ MNIST ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹ì„ ìœ„í•œ Spiking Neural Network (SNN)ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ê°œìš”\n",
    "- ì •ì  ì´ë¯¸ì§€(MNIST)ë¥¼ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "- LIF ë‰´ëŸ°ì„ ì‚¬ìš©í•œ SNN ëª¨ë¸ êµ¬ì¶•\n",
    "- ìŠ¤íŒŒì´í¬ í™œë™ ì‹œê°í™”\n",
    "- M1 Mac MPS ê°€ì† í™œìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì • (M1 Mac MPS í™œìš©)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "batch_size = 128\n",
    "data_path = './data'\n",
    "num_steps = 25    # ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì‹œê°„ ìŠ¤í… ìˆ˜)\n",
    "beta = 0.95       # LIF ë‰´ëŸ°ì˜ ë©¤ë¸Œë ˆì¸ ì „ì•• ê°ì‡ ìœ¨ (decay rate)\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n",
    "num_inputs = 28*28\n",
    "num_hidden = 100  # ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜\n",
    "num_outputs = 10  # MNIST í´ë˜ìŠ¤ ìˆ˜\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Time steps: {num_steps}\")\n",
    "print(f\"- Beta (decay): {beta}\")\n",
    "print(f\"- Hidden neurons: {num_hidden}\")\n",
    "print(f\"- Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë³€í™˜: í…ì„œí™”, ì •ê·œí™”\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0,), (1.0,))\n",
    "])\n",
    "\n",
    "# MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
    "mnist_train = datasets.MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(mnist_train)}\")\n",
    "print(f\"Test dataset size: {len(mnist_test)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ MNIST ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "def visualize_mnist_samples(loader, num_samples=8):\n",
    "    \"\"\"MNIST ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = images[i].squeeze().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('MNIST Sample Images')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_mnist_samples(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ìŠ¤íŒŒì´í¬ ì¸ì½”ë”© ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spike_encoding(image, label, num_steps=25):\n",
    "    \"\"\"ì´ë¯¸ì§€ì˜ ìŠ¤íŒŒì´í¬ ì¸ì½”ë”©ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # Rate codingì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    # í”½ì…€ ê°’ì— ë¹„ë¡€í•˜ì—¬ ìŠ¤íŒŒì´í¬ ë°œìƒ í™•ë¥  ê²°ì •\n",
    "    spike_data = snn.spikegen.rate(image.squeeze(), num_steps=num_steps, gain=1, offset=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # ì›ë³¸ ì´ë¯¸ì§€\n",
    "    axes[0].imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Label: {label})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # ì´ ìŠ¤íŒŒì´í¬ ìˆ˜ (ì‹œê°„ ì¶•ìœ¼ë¡œ í•©ì‚°)\n",
    "    total_spikes = spike_data.sum(dim=0).numpy()\n",
    "    axes[1].imshow(total_spikes, cmap='hot', origin='lower')\n",
    "    axes[1].set_title(f'Total Spike Count\\n(over {num_steps} timesteps)')\n",
    "    axes[1].set_xlabel('Pixel X')\n",
    "    axes[1].set_ylabel('Pixel Y')\n",
    "    \n",
    "    # íŠ¹ì • í”½ì…€ì˜ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ (ì¤‘ì•™ í”½ì…€)\n",
    "    center_pixel_spikes = spike_data[:, 14, 14].numpy()\n",
    "    time_steps = np.arange(num_steps)\n",
    "    axes[2].stem(time_steps, center_pixel_spikes, basefmt=\" \")\n",
    "    axes[2].set_title('Spike Sequence\\n(Center Pixel 14,14)')\n",
    "    axes[2].set_xlabel('Time Step')\n",
    "    axes[2].set_ylabel('Spike (1=fired)')\n",
    "    axes[2].set_ylim(-0.1, 1.1)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return spike_data\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¡œ ìŠ¤íŒŒì´í¬ ì¸ì½”ë”© ì‹œì—°\n",
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "spike_example = visualize_spike_encoding(sample_images[0], sample_labels[0].item(), num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SNN ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNNet(nn.Module):\n",
    "    \"\"\"ê°„ë‹¨í•œ 2ì¸µ SNN ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, beta):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.beta = beta\n",
    "\n",
    "        # ë ˆì´ì–´ ì •ì˜\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"ìˆœì „íŒŒ í•¨ìˆ˜\n",
    "        \n",
    "        Args:\n",
    "            x: ì…ë ¥ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ (num_steps, batch_size, num_inputs)\n",
    "            \n",
    "        Returns:\n",
    "            cur_out: ì¶œë ¥ì¸µì˜ ë©¤ë¸Œë ˆì¸ ì „ì•• ì‹œí€€ìŠ¤\n",
    "            spk_out: ì¶œë ¥ì¸µì˜ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤\n",
    "            spk_hidden: ì€ë‹‰ì¸µì˜ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ (ì‹œê°í™”ìš©)\n",
    "        \"\"\"\n",
    "        # ë©¤ë¸Œë ˆì¸ ì „ì•• ì´ˆê¸°í™”\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        cur_list = []  # ì¶œë ¥ì¸µ ì „ë¥˜ ê¸°ë¡\n",
    "        spk_list = []  # ì¶œë ¥ì¸µ ìŠ¤íŒŒì´í¬ ê¸°ë¡\n",
    "        spk_hidden_list = []  # ì€ë‹‰ì¸µ ìŠ¤íŒŒì´í¬ ê¸°ë¡\n",
    "\n",
    "        # ì‹œê°„ ìŠ¤í…ë³„ ìˆœì „íŒŒ\n",
    "        for step in range(x.size(0)):  # num_steps\n",
    "            cur1 = self.fc1(x[step].view(-1, self.num_inputs))  # Flatten í›„ FC1\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)  # LIF ë‰´ëŸ° 1\n",
    "            cur2 = self.fc2(spk1)  # FC2\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)  # LIF ë‰´ëŸ° 2\n",
    "\n",
    "            cur_list.append(cur2)\n",
    "            spk_list.append(spk2)\n",
    "            spk_hidden_list.append(spk1)\n",
    "\n",
    "        return torch.stack(cur_list, dim=0), torch.stack(spk_list, dim=0), torch.stack(spk_hidden_list, dim=0)\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "net = SNNNet(num_inputs, num_hidden, num_outputs, beta).to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in net.parameters())} parameters\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ìš© ë³€ìˆ˜\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "train_acc_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í‰ê°€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(net, data_loader, device, num_steps):\n",
    "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            # ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ ìƒì„± (ê°„ë‹¨í•œ ë°˜ë³µ ë°©ì‹)\n",
    "            spike_data = images.unsqueeze(0).repeat(num_steps, 1, 1, 1, 1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # ìˆœì „íŒŒ\n",
    "            cur_out, spk_out, _ = net(spike_data)\n",
    "            \n",
    "            # ì˜ˆì¸¡ (ì´ ìŠ¤íŒŒì´í¬ ìˆ˜ ê¸°ë°˜)\n",
    "            total_spikes = spk_out.sum(dim=0)  # ì‹œê°„ ì¶•ìœ¼ë¡œ í•©ì‚°\n",
    "            _, predicted = total_spikes.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "def print_batch_accuracy(net, images, labels, device, num_steps, train=False):\n",
    "    \"\"\"ë°°ì¹˜ ì •í™•ë„ ì¶œë ¥ í•¨ìˆ˜\"\"\"\n",
    "    accuracy = evaluate_single_batch(net, images, labels, device, num_steps)\n",
    "    if train:\n",
    "        print(f\"  Batch Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_single_batch(net, images, labels, device, num_steps):\n",
    "    \"\"\"ë‹¨ì¼ ë°°ì¹˜ í‰ê°€\"\"\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        spike_data = images.unsqueeze(0).repeat(num_steps, 1, 1, 1, 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        cur_out, spk_out, _ = net(spike_data)\n",
    "        total_spikes = spk_out.sum(dim=0)\n",
    "        _, predicted = total_spikes.max(1)\n",
    "        \n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "    net.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    epoch_loss = 0\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ ìƒì„± (ê°„ë‹¨í•œ ë°˜ë³µ ë°©ì‹)\n",
    "        # ì‹¤ì œë¡œëŠ” snn.spikegen.rate()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "        spike_data = images.unsqueeze(0).repeat(num_steps, 1, 1, 1, 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ìˆœì „íŒŒ\n",
    "        cur_out, spk_out, spk_hidden = net(spike_data)\n",
    "\n",
    "        # ì†ì‹¤ ê³„ì‚° (ì´ ìŠ¤íŒŒì´í¬ ìˆ˜ ê¸°ë°˜)\n",
    "        loss = loss_fn(spk_out.sum(dim=0), labels)\n",
    "\n",
    "        # ì—­ì „íŒŒ ë° ìµœì í™”\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        epoch_loss += loss.item()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        # ì •í™•ë„ ê³„ì‚°\n",
    "        with torch.no_grad():\n",
    "            total_spikes = spk_out.sum(dim=0)\n",
    "            _, predicted = total_spikes.max(1)\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if (i + 1) % 100 == 0:\n",
    "            batch_acc = (predicted == labels).sum().item() / labels.size(0)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(train_loader)}\")\n",
    "            print(f\"  Loss: {loss.item():.4f}, Batch Acc: {batch_acc*100:.2f}%\")\n",
    "\n",
    "    # ì—í¬í¬ ì¢…ë£Œ ì‹œ í‰ê°€\n",
    "    train_accuracy = num_correct / num_samples\n",
    "    test_accuracy = evaluate_model(net, test_loader, device, num_steps)\n",
    "    \n",
    "    train_acc_hist.append(train_accuracy)\n",
    "    test_acc_hist.append(test_accuracy)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(f\"  Average Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    print(f\"  Train Accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¼ì • ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ì†ì‹¤ ê·¸ë˜í”„\n",
    "axes[0].plot(loss_hist)\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì •í™•ë„ ê·¸ë˜í”„\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "axes[1].plot(epochs_range, [acc*100 for acc in train_acc_hist], 'b-', label='Train Accuracy')\n",
    "axes[1].plot(epochs_range, [acc*100 for acc in test_acc_hist], 'r-', label='Test Accuracy')\n",
    "axes[1].set_title(\"Accuracy per Epoch\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ í‘œì‹œ\n",
    "final_train_acc = train_acc_hist[-1] * 100\n",
    "final_test_acc = test_acc_hist[-1] * 100\n",
    "axes[2].bar(['Train', 'Test'], [final_train_acc, final_test_acc], \n",
    "           color=['blue', 'red'], alpha=0.7)\n",
    "axes[2].set_title(\"Final Accuracy\")\n",
    "axes[2].set_ylabel(\"Accuracy (%)\")\n",
    "axes[2].set_ylim(0, 100)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "axes[2].text(0, final_train_acc + 1, f'{final_train_acc:.1f}%', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "axes[2].text(1, final_test_acc + 1, f'{final_test_acc:.1f}%', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Train Accuracy: {final_train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {final_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ìŠ¤íŒŒì´í¬ í™œë™ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spike_activity(net, images, labels, device, num_steps, sample_idx=0):\n",
    "    \"\"\"íŠ¹ì • ìƒ˜í”Œì— ëŒ€í•œ ìŠ¤íŒŒì´í¬ í™œë™ ì‹œê°í™”\"\"\"\n",
    "    net.eval()\n",
    "    \n",
    "    # ë‹¨ì¼ ìƒ˜í”Œ ì„ íƒ\n",
    "    sample_image = images[sample_idx:sample_idx+1]\n",
    "    sample_label = labels[sample_idx:sample_idx+1]\n",
    "    \n",
    "    # ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    spike_data = sample_image.unsqueeze(0).repeat(num_steps, 1, 1, 1, 1).to(device)\n",
    "    sample_label = sample_label.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cur_out, spk_out, spk_hidden = net(spike_data)\n",
    "    \n",
    "    # CPUë¡œ ì´ë™\n",
    "    spk_out_cpu = spk_out.cpu().numpy()  # (num_steps, batch_size, num_outputs)\n",
    "    spk_hidden_cpu = spk_hidden.cpu().numpy()  # (num_steps, batch_size, num_hidden)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # ì›ë³¸ ì´ë¯¸ì§€\n",
    "    axes[0,0].imshow(sample_image.squeeze().numpy(), cmap='gray')\n",
    "    axes[0,0].set_title(f'Input Image (Label: {sample_label.item()})')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # ì¶œë ¥ ë‰´ëŸ° ìŠ¤íŒŒì´í¬ í™œë™\n",
    "    output_spikes = spk_out_cpu[:, 0, :].T  # (num_outputs, num_steps)\n",
    "    im1 = axes[0,1].imshow(output_spikes, cmap='Greys', aspect='auto')\n",
    "    axes[0,1].set_title('Output Neuron Spikes')\n",
    "    axes[0,1].set_xlabel('Time Step')\n",
    "    axes[0,1].set_ylabel('Output Neuron Index (0-9)')\n",
    "    axes[0,1].set_yticks(range(10))\n",
    "    plt.colorbar(im1, ax=axes[0,1], label='Spike (1=fired)')\n",
    "    \n",
    "    # ì€ë‹‰ì¸µ ë‰´ëŸ° ìŠ¤íŒŒì´í¬ í™œë™ (ì²˜ìŒ 20ê°œë§Œ)\n",
    "    hidden_spikes = spk_hidden_cpu[:, 0, :20].T  # (20, num_steps)\n",
    "    im2 = axes[1,0].imshow(hidden_spikes, cmap='Greys', aspect='auto')\n",
    "    axes[1,0].set_title('Hidden Layer Spikes (first 20 neurons)')\n",
    "    axes[1,0].set_xlabel('Time Step')\n",
    "    axes[1,0].set_ylabel('Hidden Neuron Index')\n",
    "    plt.colorbar(im2, ax=axes[1,0], label='Spike (1=fired)')\n",
    "    \n",
    "    # ì¶œë ¥ ë‰´ëŸ°ë³„ ì´ ìŠ¤íŒŒì´í¬ ìˆ˜\n",
    "    total_spikes_per_neuron = output_spikes.sum(axis=1)\n",
    "    predicted_label = np.argmax(total_spikes_per_neuron)\n",
    "    \n",
    "    bars = axes[1,1].bar(range(10), total_spikes_per_neuron)\n",
    "    axes[1,1].set_title(f'Total Spikes per Output Neuron\\n(Predicted: {predicted_label})')\n",
    "    axes[1,1].set_xlabel('Output Neuron Index')\n",
    "    axes[1,1].set_ylabel('Total Spikes')\n",
    "    axes[1,1].set_xticks(range(10))\n",
    "    \n",
    "    # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ê°•ì¡°\n",
    "    bars[predicted_label].set_color('red')\n",
    "    bars[sample_label.item()].set_edgecolor('blue')\n",
    "    bars[sample_label.item()].set_linewidth(3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_label == sample_label.item()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë“¤ì— ëŒ€í•œ ìŠ¤íŒŒì´í¬ í™œë™ ì‹œê°í™”\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "print(\"Visualizing spike activity for test samples...\")\n",
    "for i in range(min(3, len(test_images))):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    is_correct = visualize_spike_activity(net, test_images, test_labels, device, num_steps, i)\n",
    "    print(f\"Prediction {'correct' if is_correct else 'incorrect'}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "model_path = 'snn_mnist_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_hist': loss_hist,\n",
    "    'train_acc_hist': train_acc_hist,\n",
    "    'test_acc_hist': test_acc_hist,\n",
    "    'hyperparameters': {\n",
    "        'num_inputs': num_inputs,\n",
    "        'num_hidden': num_hidden,\n",
    "        'num_outputs': num_outputs,\n",
    "        'beta': beta,\n",
    "        'num_steps': num_steps,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Final test accuracy: {test_acc_hist[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²°ë¡ \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” snnTorchë¥¼ ì‚¬ìš©í•˜ì—¬ MNIST ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹ì„ ìœ„í•œ ê¸°ë³¸ì ì¸ SNNì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” í•™ìŠµ ë‚´ìš©:\n",
    "1. **ìŠ¤íŒŒì´í¬ ì¸ì½”ë”©**: ì •ì  ì´ë¯¸ì§€ë¥¼ ìŠ¤íŒŒì´í¬ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "2. **LIF ë‰´ëŸ°**: Leaky Integrate-and-Fire ë‰´ëŸ°ì˜ ë™ì‘ ì›ë¦¬\n",
    "3. **ì‹œê°„ ì—­í•™**: SNNì—ì„œ ì‹œê°„ ì°¨ì›ì˜ ì¤‘ìš”ì„±\n",
    "4. **ìŠ¤íŒŒì´í¬ ê¸°ë°˜ í•™ìŠµ**: ëŒ€ë¦¬ ê¸°ìš¸ê¸°ë¥¼ í†µí•œ ì—­ì „íŒŒ\n",
    "5. **ì‹œê°í™”**: ìŠ¤íŒŒì´í¬ í™œë™ íŒ¨í„´ ë¶„ì„\n",
    "\n",
    "### ê°œì„  ë°©í–¥:\n",
    "- ë” ì •êµí•œ ìŠ¤íŒŒì´í¬ ì¸ì½”ë”© ê¸°ë²• (Rate coding, Temporal coding)\n",
    "- ë‹¤ì–‘í•œ ë‰´ëŸ° ëª¨ë¸ (Synaptic, RSynaptic ë“±)\n",
    "- CNN êµ¬ì¡°ì™€ì˜ ê²°í•©\n",
    "- ì‹¤ì œ ë‰´ë¡œëª¨í”½ ë°ì´í„°ì…‹ (N-MNIST, DVS ë“±) í™œìš©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}